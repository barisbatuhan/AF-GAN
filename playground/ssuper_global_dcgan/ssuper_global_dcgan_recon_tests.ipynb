{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "os.path.dirname(sys.executable)\n",
    "# sys.path.append('/kuacc/users/ckoksal20/COMP547Project/SSuperGAN/')\n",
    "sys.path.append('/kuacc/users/baristopal20/SSuperGAN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets.random_dataset import RandomDataset\n",
    "from data.datasets.golden_panels import GoldenPanelsDataset\n",
    "from data.augment import get_PIL_image\n",
    "\n",
    "from networks.ssuper_model import SSuperModel\n",
    "from networks.models import SSuperVAE, SSuperGlobalDCGAN\n",
    "from utils.config_utils import read_config, Config\n",
    "from utils.logging_utils import *\n",
    "from utils.plot_utils import *\n",
    "from utils import pytorch_util as ptu\n",
    "\n",
    "from configs.base_config import *\n",
    "from functional.losses.elbo import elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(Config.SSUPERGLOBALDCGAN)\n",
    "golden_age_config = read_config(Config.GOLDEN_AGE)\n",
    "model_path = \"ckpts/lstm_ssuper_global_dcgan_model-checkpoint-epoch88.pth\"\n",
    "# N_SAMPLES = 1280 # 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "net = SSuperModel(\n",
    "    backbone=config.backbone,\n",
    "    embed_dim=256,\n",
    "    latent_dim=256,\n",
    "    img_size=64,\n",
    "    panel_size=(256, 256),\n",
    "    use_lstm=False,\n",
    "    gen_channels=config.gen_channels,\n",
    "    local_disc_channels=config.local_disc_channels,\n",
    "    global_disc_channels=config.global_disc_channels,\n",
    "    seq_size=config.seq_size,\n",
    "    masked_first=config.masked_first,\n",
    "    enc_choice=None,                  # options: [\"vae\", None]. If \"vae\", then gen. should be also vae\n",
    "    gen_choice=\"stylegan2\",               # options: [\"dcgan\", \"stylegan2\", \"vae\"]\n",
    "    local_disc_choice=\"stylegan2\",        # options: [\"dcgan\", \"stylegan2\", inpainting\", None]\n",
    "    global_disc_choice=\"stylegan2\",\n",
    ").cuda()\n",
    "\n",
    "x = torch.randn(12, 3, 3, 256, 256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 64, 64])\n",
      "torch.Size([12, 1])\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, _ = net(x, f=\"seq_encode\")\n",
    "    out = net(out, f=\"generate\")\n",
    "    print(out.shape)\n",
    "    out = net(out, f=\"discriminate\", local=True)\n",
    "    print(out.shape)\n",
    "    out = net(x[:,0,:,:,:], f=\"discriminate\", local=False)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSuperGlobalDCGAN(\n",
    "    backbone=config.backbone,\n",
    "    embed_dim=config.embed_dim,\n",
    "    latent_dim=config.latent_dim,\n",
    "    img_size=config.img_size,\n",
    "    panel_size=config.panel_size,\n",
    "    use_lstm=config.use_lstm,\n",
    "    gen_channels=config.gen_channels,\n",
    "    local_disc_channels=config.local_disc_channels,\n",
    "    global_disc_channels=config.global_disc_channels,\n",
    "    seq_size=config.seq_size,\n",
    "    lstm_conv=config.lstm_conv,\n",
    "    lstm_bidirectional=config.lstm_bidirectional,\n",
    "    lstm_hidden=config.lstm_hidden,\n",
    "    lstm_dropout=config.lstm_dropout,\n",
    "    fc_hidden_dims=config.fc_hidden_dims,\n",
    "    fc_dropout=config.fc_dropout,\n",
    "    num_lstm_layers=config.num_lstm_layers,\n",
    "    masked_first=config.masked_first)\n",
    "\n",
    "if config.parallel:\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "net.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "net = net.cuda().eval()\n",
    "\n",
    "dataset = GoldenPanelsDataset(golden_age_config.panel_path,\n",
    "                              golden_age_config.sequence_path, \n",
    "                              config.panel_size,\n",
    "                              config.img_size, \n",
    "                              augment=False, \n",
    "                              mask_val=1, # mask with white color for 1 and black color for 0\n",
    "                              mask_all=False, # masks faces from all panels and returns all faces\n",
    "                              return_mask=True,\n",
    "                              return_mask_coordinates=True,\n",
    "                              train_test_ratio=golden_age_config.train_test_ratio,\n",
    "                              train_mode=False,\n",
    "                              limit_size=-1)\n",
    "\n",
    "dataset.data = [\n",
    "    [['593/29_6.jpg', '593/30_0.jpg', '593/30_1.jpg'], [[[29, 0, 532, 503], [183, 51, 281, 149]], [[7, 0, 543, 536], [90, 229, 206, 345]], \n",
    "                                                        [[15, 0, 552, 537], [186, 192, 298, 304]]]], # animal\n",
    "    \n",
    "    [['3918/34_0.jpg', '3918/34_1.jpg', '3918/34_2.jpg'], [[[31, 0, 570, 539], [117, 278, 239, 400]], [[0, 14, 506, 520], [155, 181, 195, 221]],\n",
    "                                                           [[18, 0, 514, 496], [121, 274, 229, 382]]]], # woman\n",
    "    \n",
    "    [['211/20_0.jpg', '211/20_1.jpg', '211/20_2.jpg'], [[[612, 0, 1165, 553], [1041, 188, 1105, 252]], [[20, 0, 589, 569], [386, 189, 462, 265]],\n",
    "                                                        [[39, 0, 595, 556], [95, 175, 289, 369]]]], # bad man\n",
    "    \n",
    "    [['915/30_1.jpg', '915/30_2.jpg', '915/30_3.jpg'], [[[0, 7, 419, 426], [127, 205, 255, 333]], [[0, 59, 356, 415], [160, 258, 282, 380]],\n",
    "                                                        [[0, 82, 264, 346], [89, 228, 193, 332]]]], # good man\n",
    "\n",
    "    [['3513/21_5.jpg', '3513/21_6.jpg', '3513/22_0.jpg'], [[[59, 0, 611, 552], [199, 180, 369, 350]], [[0, 17, 533, 550], [90, 206, 164, 280]],\n",
    "                                                           [[0, 69, 395, 464], [73, 230, 185, 342]]]], # good man\n",
    "\n",
    "    [['2464/73_4.jpg', '2464/73_5.jpg', '2464/73_6.jpg'], [[[131, 0, 743, 612], [225, 206, 361, 342]], [[168, 0, 890, 722], [602, 316, 658, 372]],\n",
    "                                                           [[73, 0, 740, 667], [185, 266, 331, 412]]]], # mask problem\n",
    "]\n",
    "\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, mask, coord in data_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu_z, _ = net(x.cuda(), f=\"seq_encode\")\n",
    "        mu_z = mu_z.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_recon = net(mu_z, f=\"generate\")\n",
    "        recon_global, gt_global = net.module.create_global_images(copy.deepcopy(x), y, y_recon, coord)\n",
    "        plot_panels_and_faces(torch.cat([x[:,:2,:,:,:], gt_global.cpu().unsqueeze(1)], dim=1), y, y_recon, recon_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "ctr, limit = 0, 5\n",
    "x_arr = []\n",
    "y_arr = []\n",
    "mask_arr = []\n",
    "coord_arr = []\n",
    "for _ in range(limit):\n",
    "    x, y, mask, coord = next(dl_iter)\n",
    "    x_arr.append(x)\n",
    "    y_arr.append(y)\n",
    "    mask_arr.append(mask)\n",
    "    coord_arr.append(coord)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mu_z, _ = net(x.cuda(), f=\"seq_encode\")\n",
    "        mu_z = mu_z.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_recon = net(mu_z, f=\"generate\")\n",
    "        recon_global, gt_global = net.module.create_global_images(copy.deepcopy(x), y, y_recon, coord)\n",
    "        \n",
    "    plot_panels_and_faces(torch.cat([x[:,:2,:,:,:], gt_global.cpu().unsqueeze(1)], dim=1), y, y_recon, recon_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_lstm and not config.lstm_conv:\n",
    "    net.module.seq_encoder.lstm.train()\n",
    "\n",
    "x = x_arr[2]\n",
    "# x, y, mask, coord = next(dl_iter)\n",
    "draw_saliency(net, x, y)\n",
    "\n",
    "if config.use_lstm and not config.lstm_conv:\n",
    "    net.module.seq_encoder.lstm.eval()\n",
    "\n",
    "backbone = net.seq_encoder.embedder if not config.parallel else net.module.seq_encoder.embedder\n",
    "draw_backbone_saliency(backbone, x, 0)\n",
    "draw_backbone_saliency(backbone, x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss()(torch.Tensor([0.7, 0.3, 0.1, 0.95]), torch.Tensor([1.2, 0.2, -0.3, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
